{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from collections import defaultdict, OrderedDict\n",
    "from itertools import chain, product\n",
    "import string\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "from urllib.parse import quote\n",
    "import urllib3\n",
    "from scipy.spatial.distance import cosine\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from wrappers import wcrft\n",
    "\n",
    "import plwn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wordnet:\n",
    "    \n",
    "    def __init__(self, wordnet):\n",
    "        self._wordnet = wordnet\n",
    "        self._lexical_units = self._valid_lexical_units()\n",
    "        \n",
    "    def _valid_lexical_units(self):\n",
    "        return [(lu.lemma, lu.synset, lu.definition) for lu in tqdm(self._wordnet.lexical_units(), \n",
    "                                                                    desc='Filtering polish') if lu.is_polish]\n",
    "    \n",
    "    def senses(self, word):\n",
    "        return [lu for lu in self._lexical_units if lu[0] == word]\n",
    "    \n",
    "    def senses_verbose(self, word):\n",
    "        def reshape_senses(senses):\n",
    "            if senses:\n",
    "                return list(np.asarray(senses)[:,0].astype(int))\n",
    "            return []\n",
    "        \n",
    "        def verbose_part(sense):    \n",
    "            return dict([[k, reshape_senses(v)]\n",
    "                         for k, v in sense[1].to_dict()['related'].items()])\n",
    "        \n",
    "        return [(*sense, verbose_part(sense)) \n",
    "                for sense in self.senses(word)]\n",
    "    \n",
    "    @property\n",
    "    def orig(self):\n",
    "        return self._wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopWord:\n",
    "    \n",
    "    def __init__(self, file):\n",
    "        self._stop_words = self._load_file(file)\n",
    "    \n",
    "    @property\n",
    "    def stop_words(self):\n",
    "        return self._stop_words\n",
    "    \n",
    "    def _load_file(self, file):\n",
    "        f = open(file, encoding='utf-8')\n",
    "        lines = f.read().splitlines()\n",
    "        return lines\n",
    "    \n",
    "    def is_stop_word(self, word):\n",
    "        return word.lower() in self._stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoteWordEmbedding:\n",
    "    \n",
    "    def __init__(self, address):\n",
    "        self._address = address\n",
    "        self._http = urllib3.PoolManager()\n",
    "        \n",
    "    def get_embedding(self, word):\n",
    "        word = quote(word)\n",
    "        target = '{addr}/word_emb/{word}'.format(addr=self._address, word=word)\n",
    "        data = self._http.request('GET', target)\n",
    "        if data.status == 200:\n",
    "            return ast.literal_eval(data.data.decode('ascii'))\n",
    "        else:\n",
    "            raise Exception('A problem occured during getting an embedding for a word...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoteSenseEmbedding:\n",
    "    \n",
    "    def __init__(self, address):\n",
    "        self._address = address\n",
    "        self._http = urllib3.PoolManager()\n",
    "        \n",
    "    def get_embedding(self, synset_id):\n",
    "        target = '{addr}/sense_emb/{synset_id}'.format(addr=self._address, synset_id=synset_id)\n",
    "        data = self._http.request('GET', target)\n",
    "        if data.status == 200:\n",
    "            return ast.literal_eval(data.data.decode('ascii'))\n",
    "        else:\n",
    "            raise Exception('A problem occured during getting an embedding for a synset id...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b7e989c01f4add8938031dc7a96dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Filtering polish', max=504102, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "remote_embeddings_address = 'http://10.17.5.15:4000'\n",
    "stop_words_file = 'polish.stopwords.txt'\n",
    "\n",
    "sw = StopWord(stop_words_file)\n",
    "wn = Wordnet(plwn.load_default())\n",
    "we = RemoteWordEmbedding(remote_embeddings_address)\n",
    "se = RemoteSenseEmbedding(remote_embeddings_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str, sw):\n",
    "    text = ' '.join(word for word in text.split() if not sw.is_stop_word(word))\n",
    "    return text.translate(str.maketrans({key: None for key in string.punctuation}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_text(text: str):\n",
    "    return wcrft.tag(clean_text(text, sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmas(tagger_out):\n",
    "    return [v[0] for v in tagger_out.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_senses(word: str):\n",
    "    return dict([[x[1].id, x[2]] for x in wn.senses(word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gloss(word: str, synset_id: int):\n",
    "    # TODO: add tagger\n",
    "    return get_senses(word).get(synset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_by_senses_count(lemmas):\n",
    "    sorted_words = {}\n",
    "    \n",
    "    for lemma in lemmas:\n",
    "        count = len(wn.senses(lemma))\n",
    "        \n",
    "        if count:\n",
    "            sorted_words[lemma] = count\n",
    "            \n",
    "    return OrderedDict(sorted(sorted_words.items(), key=lambda x: x[1])).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['wzgórze', 'obronny', 'zamek', 'wysoki'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'obronny zamek na wysokim wzgórzu'\n",
    "sorted_by_senses_count(get_lemmas(tag_text(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK :)\n",
    "# def related_syn_ids(word, synset_id, *relations):\n",
    "#     related = [related[3] for related in wn.senses_verbose(word) if related[1].id == synset_id]\n",
    "#     if related:\n",
    "#         related = chain(*[v for k, v in related[0].items() \n",
    "#                           for relation in relations if k.startswith(relation)])\n",
    "#     return list(map(int, related))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK :)\n",
    "# def glosses_from_syn_ids(syn_ids):\n",
    "#     def only_polish_def(syn_id):\n",
    "#         data = wn.orig.synset_by_id(syn_id)\n",
    "#         return data.to_dict()['units'][0]['definition'] if data.is_polish else None\n",
    "#     glosses = filter(lambda x: x not in [None, '.'], map(only_polish_def, syn_ids))\n",
    "#     return list(glosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def C_w(W_wout_W, disambiguated):\n",
    "#     def _get_vector(w):\n",
    "#         return se[disambiguated[w]] if w in disambiguated.keys() else we.wv[w]\n",
    "#     return np.average(list(map(_get_vector, W_wout_W)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def G_s(word, synset_id, use_related=True, *relations):\n",
    "#     if use_related:\n",
    "#         synset_ids = related_syn_ids(word, synset_id, *relations)\n",
    "#         glosses = glosses_from_syn_ids(synset_ids)\n",
    "#         gloss = list(chain(*filter(lambda words: len(words) == 1,\n",
    "#                             chain(map(list, chain(*map(get_words, glosses)))))))\n",
    "#     else:\n",
    "#         gloss = get_gloss(word, synset_id)\n",
    "#     return np.average(list(map(we.wv.get_vector, gloss)), axis=0) if gloss else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def wsd(W, use_related=True, relations=['hiperonimia', 'synonimia']):\n",
    "    \n",
    "#     best_scores = []\n",
    "    \n",
    "#     for lemmas_set in tqdm(list(product(*get_words(W))), 'Lemmas set'):\n",
    "#         best_senses = {}\n",
    "        \n",
    "#         W_temp = list(sorted_by_senses_count(lemmas_set)) # sortowanie po liczbie sensów\n",
    "#         sum_scores = 0.0\n",
    "        \n",
    "#         for w in tqdm(W_temp, 'Word', leave=False):\n",
    "#             word_score = {}\n",
    "            \n",
    "#             best_sense, best_score = None, 0.0\n",
    "\n",
    "#             W_wout_w = W_temp.copy() \n",
    "#             W_wout_w.remove(w) \n",
    "#             c_w = C_w(W_wout_w, best_senses)\n",
    "\n",
    "#             for word, synset, gloss in wn.senses(w):\n",
    "\n",
    "#                 if se.get(synset.id) is None:\n",
    "#                     continue\n",
    "\n",
    "#                 g_s = G_s(word, synset.id, use_related, *relations) \n",
    "\n",
    "#                 first_cos = 1 - cosine(g_s, c_w) \n",
    "#                 second_cos = 1 - cosine(se[synset.id], c_w)\n",
    "#                 score = first_cos + second_cos # suma podobieństw cos\n",
    "\n",
    "#                 word_score[synset.id] = score\n",
    "\n",
    "#                 if score > best_score:\n",
    "#                     best_score = score\n",
    "#                     best_sense = synset.id\n",
    "                    \n",
    "#             if best_sense:\n",
    "#                 best_senses[w] = best_sense\n",
    "                \n",
    "#             sum_scores += best_score\n",
    "            \n",
    "#         best_scores.append((sum_scores, best_senses))\n",
    "    \n",
    "#     best_senses = sorted(best_scores, key=lambda data: data[0], reverse=True)[0][1]\n",
    "#     return dict(map(lambda data: (data[0], get_gloss(*data)), best_senses.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wsd_out = wsd('Język (łac. lingua) – wieloczynnościowy narząd, twór mięśniowy jamy gębowej kręgowców. Głównym zadaniem języka jest podsuwanie pokarmu pod zęby, mieszanie pokarmu w czasie żucia i przesuwanie kęsów pokarmu do gardła, lecz niektóre gatunki używają go również do innych celów.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wsd_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
